{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f6feb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import bs4\n",
    "from urllib.request import urlopen\n",
    "import urllib\n",
    "import time\n",
    "import tkinter as tk\n",
    "from tkinter import messagebox\n",
    "import datetime\n",
    "import time\n",
    "import re\n",
    "import numpy as np\n",
    "def kortime2engtime(time_string):\n",
    "    today = datetime.datetime.today()\n",
    "    number = int(re.findall('\\d+', time_string)[0])\n",
    "    kortime = ''.join(re.findall(\"[^0-9]\",time_string))\n",
    "    if kortime == '일' :\n",
    "        before_date = datetime.timedelta(days=number)\n",
    "    elif kortime == '분':\n",
    "        before_date= datetime.timedelta(minutes=number)\n",
    "    elif kortime == '시간' :\n",
    "        before_date = datetime.timedelta(hours=number)\n",
    "    elif kortime == '초' : \n",
    "        before_date = datetime.timedelta(seconds=number)\n",
    "    else:\n",
    "        return time_string\n",
    "    save_date = (today-before_date).strftime('%Y.%m.%d')\n",
    "    return save_date\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import bs4\n",
    "from urllib.request import urlopen\n",
    "import urllib\n",
    "import time\n",
    "import tkinter as tk\n",
    "from tkinter import messagebox\n",
    "import datetime\n",
    "import time\n",
    "import re\n",
    "def kortime2engtime(time_string):\n",
    "    today = datetime.datetime.today()\n",
    "    number = int(re.findall('\\d+', time_string)[0])\n",
    "    kortime = ''.join(re.findall(\"[^0-9]\",time_string))\n",
    "    if kortime == '일' :\n",
    "        before_date = datetime.timedelta(days=number)\n",
    "    elif kortime == '분':\n",
    "        before_date= datetime.timedelta(minutes=number)\n",
    "    elif kortime == '시간' :\n",
    "        before_date = datetime.timedelta(hours=number)\n",
    "    elif kortime == '초' : \n",
    "        before_date = datetime.timedelta(seconds=number)\n",
    "    else:\n",
    "        return time_string\n",
    "    save_date = (today-before_date).strftime('%Y.%m.%d')\n",
    "    return save_date\n",
    "\n",
    "\n",
    "def naver(search):\n",
    "    searching = '''{}'''.format(search)\n",
    "    searching = searching.strip()\n",
    "    df_list = []\n",
    "    start = 1\n",
    "    keyword = re.sub('[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》]', ' ', searching).split()\n",
    "    op = re.findall('[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》]', searching)\n",
    "\n",
    "    plus = [keyword[0]]\n",
    "\n",
    "    for index, value in enumerate(op):\n",
    "        if value == '+':\n",
    "            plus.extend([keyword[index+1]])\n",
    "\n",
    "    new = keyword[0]\n",
    "    for k,o in zip(keyword[1:],op):\n",
    "        if len(keyword) ==1:\n",
    "            break\n",
    "        else:\n",
    "            new += ' '+o+k\n",
    "    word_encode = urllib.parse.quote(new)\n",
    "\n",
    "    while True:\n",
    "        url = 'https://search.naver.com/search.naver?where=news&sm=tab_jum&query={}&pd=1&start={}'.format(word_encode,\n",
    "                                                                                                          start)\n",
    "        source = urlopen(url).read()\n",
    "        source = bs4.BeautifulSoup(source, 'html.parser')\n",
    "        title_path = source.find_all('a', {'class': 'news_tit'})\n",
    "        abstract_path = source.find_all('a', {'class': 'api_txt_lines dsc_txt_wrap'})\n",
    "        date_path = source.find_all('span', {'class': 'info'})\n",
    "        for idx,v in enumerate(date_path):\n",
    "            if v.find('i'):\n",
    "                date_path.pop(idx)\n",
    "        \n",
    "        if len(title_path) == 0:\n",
    "            root = tk.Tk()\n",
    "            msg = messagebox.showerror(title=\"Naver No news\", message='please search other keyword')\n",
    "            if msg == 'ok':\n",
    "                root.destroy()\n",
    "                break\n",
    "\n",
    "        for i in range(len(title_path)):\n",
    "            title = title_path[i].get('title')\n",
    "\n",
    "            if all(key in title for key in plus):\n",
    "                abstract = abstract_path[i].text\n",
    "                link = title_path[i].get('href')\n",
    "                date_txt = date_path[i].text\n",
    "                \n",
    "                date = kortime2engtime(date_txt.split()[0])\n",
    "                df = pd.DataFrame(\n",
    "                    {'title': [title], 'abstract': [abstract], 'url': [link], 'date': [date],\n",
    "                        'engine': ['naver']})\n",
    "                df_list.append(df)\n",
    "             \n",
    "\n",
    "        current_page = source.find_all('a', {'aria-pressed': 'true'})[0].text\n",
    "        last_page = source.find_all('a', {'aria-pressed': 'false'})[-1].text\n",
    "        if last_page == '문서 저장하기':\n",
    "            break\n",
    "\n",
    "        if int(current_page) >= int(last_page):\n",
    "            break\n",
    "        else:\n",
    "            start += 10\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "        print('crawling...')\n",
    "\n",
    "    if df_list:\n",
    "            naver_df = pd.concat(df_list)\n",
    "            new = new.replace('|',' ')\n",
    "            naver_df.to_excel('''naver_{}_result.xlsx'''.format(new),index=False)\n",
    "\n",
    "            root2 = tk.Tk()\n",
    "            msg = messagebox.showinfo(title='Save msg',message='Naver Save!')\n",
    "            if msg == 'ok':\n",
    "                root2.destroy()\n",
    "            return naver_df\n",
    "    else:\n",
    "        root3 = tk.Tk()\n",
    "        msg = messagebox.showerror(title = \"Naver No news\",message = 'please search other keyword')\n",
    "        if msg == 'ok':\n",
    "            root3.destroy()\n",
    "\n",
    "\n",
    "def google(search):\n",
    "    searching = '''{}'''.format(search)\n",
    "    searching = searching.strip()\n",
    "\n",
    "    period = ' when:7d'\n",
    "    \n",
    "    keyword = re.sub('[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》]', ' ', searching).split()\n",
    "    op = re.findall('[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》]', searching)\n",
    "\n",
    "    \n",
    "    plus = [keyword[0]]\n",
    "\n",
    "    for index, value in enumerate(op):\n",
    "        if value == '+':\n",
    "            plus.extend([keyword[index+1]])\n",
    "    \n",
    "    new = keyword[0]\n",
    "    for k,o in zip(keyword[1:],op):\n",
    "        if len(keyword) ==1:\n",
    "            stop = True\n",
    "            break\n",
    "\n",
    "        else:\n",
    "            new += o+k\n",
    " \n",
    "    word_encode = urllib.parse.quote(new+period)\n",
    "    \n",
    "\n",
    "    base_url = 'https://news.google.com/'\n",
    "    url = 'https://news.google.com/search?q={}&hl=ko&gl=KR&ceid=KR:ko'.format(word_encode)\n",
    "    # print(url)\n",
    "    source = urlopen(url).read()\n",
    "    source = bs4.BeautifulSoup(source,'html.parser')\n",
    "    \n",
    "    df_list = []\n",
    "    article_list = source.find_all('a',{'class':'DY5T1d RZIKme'})\n",
    "    date_path = source.find_all('time',{'class':'WW6dff uQIVzc Sksgp'})\n",
    "\n",
    "    if len(article_list) == 0:\n",
    "        root = tk.Tk()\n",
    "        msg = messagebox.showerror(title = \"Google No news\",message = 'please search other keyword')\n",
    "        if msg == 'ok':\n",
    "            root.destroy()\n",
    "    else:\n",
    "        for i in range(len(article_list)):\n",
    "            \n",
    "            title = article_list[i].text\n",
    "            date = date_path[i].get('datetime').split('T')[0].replace('-','.')\n",
    "\n",
    "            if all(key in title for key in plus):\n",
    "                link = base_url + article_list[i].get('href')[1:]\n",
    "                df = pd.DataFrame({'title':[title],'abstract':[np.nan],'url':[link],'date':[date],'engine':['Google']})\n",
    "            \n",
    "                df_list.append(df)\n",
    "                print('crawling...')\n",
    "        \n",
    "    if df_list:\n",
    "            google_df = pd.concat(df_list)\n",
    "            \n",
    "            new = new.replace('|',' ')\n",
    "            google_df.to_excel('''google_{}_result.xlsx'''.format(new),index=False)\n",
    "\n",
    "            root2 = tk.Tk()\n",
    "            msg = messagebox.showinfo(title='Save msg',message='Google Save!')\n",
    "            if msg == 'ok':\n",
    "                root2.destroy()\n",
    "            return google_df\n",
    "    else:\n",
    "        root3 = tk.Tk()\n",
    "        msg = messagebox.showerror(title = \"Google No news\",message = 'please search other keyword')\n",
    "        if msg == 'ok':\n",
    "            root3.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21b41226",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'naver_google_crawling'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 15>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPyQt5\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mQtWidgets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPyQt5\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m uic\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnaver_google_crawling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m naver,google\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdigital_paper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m digital_news,newspim,AItimes\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtkinter\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtk\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'naver_google_crawling'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import bs4\n",
    "from urllib.request import urlopen\n",
    "import urllib\n",
    "import time\n",
    "import tkinter as tk\n",
    "from tkinter import messagebox\n",
    "import datetime\n",
    "import time\n",
    "import re\n",
    "import numpy as np\n",
    "import sys\n",
    "from PyQt5.QtWidgets import *\n",
    "from PyQt5 import uic\n",
    "from naver_google_crawling import naver,google\n",
    "from digital_paper import digital_news,newspim,AItimes\n",
    "\n",
    "import tkinter as tk\n",
    "from tkinter import messagebox\n",
    "\n",
    "def kortime2engtime(time_string):\n",
    "    today = datetime.datetime.today()\n",
    "    number = int(re.findall('\\d+', time_string)[0])\n",
    "    kortime = ''.join(re.findall(\"[^0-9]\",time_string))\n",
    "    if kortime == '일' :\n",
    "        before_date = datetime.timedelta(days=number)\n",
    "    elif kortime == '분':\n",
    "        before_date= datetime.timedelta(minutes=number)\n",
    "    elif kortime == '시간' :\n",
    "        before_date = datetime.timedelta(hours=number)\n",
    "    elif kortime == '초' : \n",
    "        before_date = datetime.timedelta(seconds=number)\n",
    "    else:\n",
    "        return time_string\n",
    "    save_date = (today-before_date).strftime('%Y.%m.%d')\n",
    "    return save_date\n",
    "\n",
    "\n",
    "def naver(search):\n",
    "    searching = '''{}'''.format(search)\n",
    "    searching = searching.strip()\n",
    "    df_list = []\n",
    "    start = 1\n",
    "    keyword = re.sub('[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》]', ' ', searching).split()\n",
    "    op = re.findall('[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》]', searching)\n",
    "\n",
    "    plus = [keyword[0]]\n",
    "\n",
    "    for index, value in enumerate(op):\n",
    "        if value == '+':\n",
    "            plus.extend([keyword[index+1]])\n",
    "\n",
    "    new = keyword[0]\n",
    "    for k,o in zip(keyword[1:],op):\n",
    "        if len(keyword) ==1:\n",
    "            break\n",
    "        else:\n",
    "            new += ' '+o+k\n",
    "    word_encode = urllib.parse.quote(new)\n",
    "\n",
    "    while True:\n",
    "        url = 'https://search.naver.com/search.naver?where=news&sm=tab_jum&query={}&pd=1&start={}'.format(word_encode,\n",
    "                                                                                                          start)\n",
    "        source = urlopen(url).read()\n",
    "        source = bs4.BeautifulSoup(source, 'html.parser')\n",
    "        title_path = source.find_all('a', {'class': 'news_tit'})\n",
    "        abstract_path = source.find_all('a', {'class': 'api_txt_lines dsc_txt_wrap'})\n",
    "        date_path = source.find_all('span', {'class': 'info'})\n",
    "        for idx,v in enumerate(date_path):\n",
    "            if v.find('i'):\n",
    "                date_path.pop(idx)\n",
    "        \n",
    "        if len(title_path) == 0:\n",
    "            root = tk.Tk()\n",
    "            msg = messagebox.showerror(title=\"Naver No news\", message='please search other keyword')\n",
    "            if msg == 'ok':\n",
    "                root.destroy()\n",
    "                break\n",
    "\n",
    "        for i in range(len(title_path)):\n",
    "            title = title_path[i].get('title')\n",
    "\n",
    "            if all(key in title for key in plus):\n",
    "                abstract = abstract_path[i].text\n",
    "                link = title_path[i].get('href')\n",
    "                date_txt = date_path[i].text\n",
    "                \n",
    "                date = kortime2engtime(date_txt.split()[0])\n",
    "                df = pd.DataFrame(\n",
    "                    {'title': [title], 'abstract': [abstract], 'url': [link], 'date': [date],\n",
    "                        'engine': ['naver']})\n",
    "                df_list.append(df)\n",
    "             \n",
    "\n",
    "        current_page = source.find_all('a', {'aria-pressed': 'true'})[0].text\n",
    "        last_page = source.find_all('a', {'aria-pressed': 'false'})[-1].text\n",
    "        if last_page == '문서 저장하기':\n",
    "            break\n",
    "\n",
    "        if int(current_page) >= int(last_page):\n",
    "            break\n",
    "        else:\n",
    "            start += 10\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "        print('crawling...')\n",
    "\n",
    "    if df_list:\n",
    "            naver_df = pd.concat(df_list)\n",
    "            new = new.replace('|',' ')\n",
    "            naver_df.to_excel('''naver_{}_result.xlsx'''.format(new),index=False)\n",
    "\n",
    "            root2 = tk.Tk()\n",
    "            msg = messagebox.showinfo(title='Save msg',message='Naver Save!')\n",
    "            if msg == 'ok':\n",
    "                root2.destroy()\n",
    "            return naver_df\n",
    "    else:\n",
    "        root3 = tk.Tk()\n",
    "        msg = messagebox.showerror(title = \"Naver No news\",message = 'please search other keyword')\n",
    "        if msg == 'ok':\n",
    "            root3.destroy()\n",
    "\n",
    "\n",
    "def google(search):\n",
    "    searching = '''{}'''.format(search)\n",
    "    searching = searching.strip()\n",
    "\n",
    "    period = ' when:7d'\n",
    "    \n",
    "    keyword = re.sub('[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》]', ' ', searching).split()\n",
    "    op = re.findall('[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》]', searching)\n",
    "\n",
    "    \n",
    "    plus = [keyword[0]]\n",
    "\n",
    "    for index, value in enumerate(op):\n",
    "        if value == '+':\n",
    "            plus.extend([keyword[index+1]])\n",
    "    \n",
    "    new = keyword[0]\n",
    "    for k,o in zip(keyword[1:],op):\n",
    "        if len(keyword) ==1:\n",
    "            stop = True\n",
    "            break\n",
    "\n",
    "        else:\n",
    "            new += o+k\n",
    " \n",
    "    word_encode = urllib.parse.quote(new+period)\n",
    "    \n",
    "\n",
    "    base_url = 'https://news.google.com/'\n",
    "    url = 'https://news.google.com/search?q={}&hl=ko&gl=KR&ceid=KR:ko'.format(word_encode)\n",
    "    # print(url)\n",
    "    source = urlopen(url).read()\n",
    "    source = bs4.BeautifulSoup(source,'html.parser')\n",
    "    \n",
    "    df_list = []\n",
    "    article_list = source.find_all('a',{'class':'DY5T1d RZIKme'})\n",
    "    date_path = source.find_all('time',{'class':'WW6dff uQIVzc Sksgp'})\n",
    "\n",
    "    if len(article_list) == 0:\n",
    "        root = tk.Tk()\n",
    "        msg = messagebox.showerror(title = \"Google No news\",message = 'please search other keyword')\n",
    "        if msg == 'ok':\n",
    "            root.destroy()\n",
    "    else:\n",
    "        for i in range(len(article_list)):\n",
    "            \n",
    "            title = article_list[i].text\n",
    "            date = date_path[i].get('datetime').split('T')[0].replace('-','.')\n",
    "\n",
    "            if all(key in title for key in plus):\n",
    "                link = base_url + article_list[i].get('href')[1:]\n",
    "                df = pd.DataFrame({'title':[title],'abstract':[np.nan],'url':[link],'date':[date],'engine':['Google']})\n",
    "            \n",
    "                df_list.append(df)\n",
    "                print('crawling...')\n",
    "        \n",
    "    if df_list:\n",
    "            google_df = pd.concat(df_list)\n",
    "            \n",
    "            new = new.replace('|',' ')\n",
    "            google_df.to_excel('''google_{}_result.xlsx'''.format(new),index=False)\n",
    "\n",
    "            root2 = tk.Tk()\n",
    "            msg = messagebox.showinfo(title='Save msg',message='Google Save!')\n",
    "            if msg == 'ok':\n",
    "                root2.destroy()\n",
    "            return google_df\n",
    "    else:\n",
    "        root3 = tk.Tk()\n",
    "        msg = messagebox.showerror(title = \"Google No news\",message = 'please search other keyword')\n",
    "        if msg == 'ok':\n",
    "            root3.destroy()\n",
    "\n",
    "def digital_news():\n",
    "    page_num = 1\n",
    "    today = datetime.datetime.now()\n",
    "    until_time = time.mktime((today-datetime.timedelta(days=7)).timetuple())\n",
    "    stop=False\n",
    "    df_list = []\n",
    "    while True:\n",
    "        url = 'https://www.etnews.com/news/section.html?id1=06&page={}'.format(page_num)\n",
    "        source = urlopen(url).read()\n",
    "        source = bs4.BeautifulSoup(source,'html.parser')\n",
    "        title_path = source.find_all('dl',{'class':'clearfix'})\n",
    "        date_path = source.find_all('dd',{'class':'date'})\n",
    "        for i in range(len(title_path)):\n",
    "            info_path = list(filter(None,title_path[i].text.split('\\n')))\n",
    "            if page_num == 1 and i == 0: pass\n",
    "            else:\n",
    "                date_tmp = date_path[i].text.split()\n",
    "                if len(date_tmp)>2:\n",
    "                    tmp = list(map(lambda x: re.sub('[각-힣]','',string=x),date_tmp))\n",
    "                    date = list(filter(None, tmp))[0]\n",
    "                else:\n",
    "                    date = re.sub('[각-힣]','',string=date_tmp[0])\n",
    "                \n",
    "                timestamp_date = time.mktime(datetime.datetime.strptime(date,'%Y.%m.%d').timetuple())\n",
    "                if timestamp_date - until_time<0:\n",
    "                    stop = True\n",
    "                    break                \n",
    "                title = info_path[0]\n",
    "                abstract = info_path[1]\n",
    "                link = title_path[i].find({'a':'href'}).get('href')[2:]\n",
    "                df = pd.DataFrame({'title':[title],'abstract':[abstract],'url':[link],'date':[date],'engine':['전자신문']})\n",
    "                df_list.append(df)\n",
    "        page_num +=1\n",
    "        if stop:\n",
    "            break\n",
    "    df = pd.concat(df_list)\n",
    "    today_str = today.strftime('%Y%m%d')\n",
    "    df.to_excel('Digital_news_{}.xlsx'.format(today_str),index=False)\n",
    "    root2 = tk.Tk()\n",
    "    msg = messagebox.showinfo(title='Save msg',message='Save!')\n",
    "    if msg == 'ok':\n",
    "        root2.destroy()\n",
    "    return df\n",
    "\n",
    "def newspim():\n",
    "    today = datetime.datetime.now()\n",
    "    df_list = []\n",
    "    until_time = time.mktime((today-datetime.timedelta(days=7)).timetuple())\n",
    "    page_num = 0\n",
    "    stop = False\n",
    "    while True:\n",
    "        url = 'https://www.newspim.com/news/lists/?category_cd=106020&page={}'.format(page_num)\n",
    "        source = urlopen(url).read()\n",
    "        source = bs4.BeautifulSoup(source,'html.parser')\n",
    "        title_path = source.find_all('strong',{'class':'subject_h'})\n",
    "        url_path = source.find_all('p',{'class':'summary'})\n",
    "        date_path = source.find_all('p',{'class':'byline'})\n",
    "        for i in range(len(title_path)):\n",
    "            date = date_path[i].text.split()[0]\n",
    "            timestamp_date = time.mktime(datetime.datetime.strptime(date,'%Y-%m-%d').timetuple())\n",
    "            if timestamp_date - until_time<0:\n",
    "                stop = True\n",
    "                break\n",
    "            title = title_path[i].text\n",
    "            abstract = url_path[i].text\n",
    "            link = url_path[i].find('a').get('href')\n",
    "            df = pd.DataFrame({'title':[title],'abstract':[abstract],'url':[link],'date':[date],'engine':['뉴스핌']})\n",
    "            df_list.append(df) \n",
    "        page_num+=20\n",
    "        if stop:\n",
    "            break\n",
    "    df = pd.concat(df_list)\n",
    "    today_str = today.strftime('%Y%m%d')\n",
    "    df.to_excel('Newspim_{}.xlsx'.format(today_str),index=False)\n",
    "    root2 = tk.Tk()\n",
    "    msg = messagebox.showinfo(title='Save msg',message='Save!')\n",
    "    if msg == 'ok':\n",
    "        root2.destroy()\n",
    "    return df\n",
    "\n",
    "def AItimes():\n",
    "    today = datetime.datetime.now()\n",
    "    page_num = 1\n",
    "    df_list= [] \n",
    "   \n",
    "    stop = False\n",
    "    \n",
    "    while True:\n",
    "        url = 'https://www.aitimes.kr/news/articleList.html?page={}&total=104&box_idxno=&sc_sub_section_code=S2N10&view_type=sm'.format(page_num)\n",
    "        source = urlopen(url).read()\n",
    "        source = bs4.BeautifulSoup(source,'html.parser')\n",
    "        title_path = source.find_all('h4',{'class':'titles'})\n",
    "        \n",
    "        for i in range(len(title_path)):\n",
    "            \n",
    "            title = title_path[i].find_all('a')[0].text\n",
    "            abstract = source.find_all('p',{'class':'lead line-6x2'})[i].find_all('a')[0].text\n",
    "            abstract = re.sub('\\s','',abstract)\n",
    "            link = \"https://www.aitimes.kr\"+source.find_all('p',{'class':'lead line-6x2'})[i].find_all('a')[0].get('href')\n",
    "\n",
    "            url_content = urlopen(link).read()\n",
    "            url_content = bs4.BeautifulSoup(url_content,'html.parser')\n",
    "            date = str(url_content.find_all('article',{'class':'item'})[0].find_all('li')[1].text[4:].split(' ')[0])\n",
    "            date_year = datetime.datetime.strptime(date,'%Y.%m.%d')\n",
    "            \n",
    "            if date_year.year != today.year:\n",
    "                stop=True\n",
    "                break\n",
    "                \n",
    "            else:\n",
    "                date = date_year\n",
    "                df = pd.DataFrame({'title':[title],'abstract':[abstract],'url':[link],'date':[date],'engine':['인공지능신문']})\n",
    "                df_list.append(df)\n",
    "                \n",
    "        page_num+=1\n",
    "        \n",
    "        if stop:\n",
    "            break\n",
    "    df = pd.concat(df_list)\n",
    "    today_str = today.strftime('%Y%m%d')\n",
    "    df.to_excel('AItimes_{}.xlsx'.format(today_str),index=False)\n",
    "    root2 = tk.Tk()\n",
    "    msg = messagebox.showinfo(title='Save msg',message='Save!')\n",
    "    if msg == 'ok':\n",
    "        root2.destroy()\n",
    "    return df\n",
    "\n",
    "\n",
    "# form_class = uic.loadUiType(r\"D:\\crawling\\crawling.ui\")[0]\n",
    "form_class = uic.loadUiType(\"crawling.ui\")[0]\n",
    "class WindowClass(QMainWindow, form_class) :\n",
    "    def __init__(self):\n",
    "    \n",
    "        super().__init__()\n",
    "        self.setupUi(self)\n",
    "\n",
    "        self.naver_check = 0\n",
    "        self.google_check = 0\n",
    "        self.keyword = ''''''\n",
    "        self.other = 0\n",
    "        self.checkBox.clicked.connect(self.check_naver)\n",
    "        self.checkBox2.clicked.connect(self.check_google)\n",
    "        self.save_button.clicked.connect(self.other_information)\n",
    "        self.Search.clicked.connect(self.crawling)\n",
    "\n",
    "    def check_naver(self):\n",
    "        if self.checkBox.isChecked() == True:\n",
    "            self.naver_check = 1\n",
    "        else:\n",
    "            self.naver_check = 0\n",
    "        print('naver check!')\n",
    "    def check_google(self):\n",
    "        if self.checkBox2.isChecked() == True:\n",
    "            self.google_check = 1\n",
    "        else:\n",
    "            self.google_check = 0\n",
    "        print('google check!')\n",
    "    def other_information(self):\n",
    "        self.other = 1\n",
    "    def crawling(self):\n",
    "        self.keyword = self.textEdit.toPlainText()\n",
    "        df = pd.DataFrame()\n",
    "        if self.naver_check:\n",
    "            naver_df = naver(self.keyword)\n",
    "            if len(naver_df):\n",
    "                df = pd.concat([df,naver_df])\n",
    "        elif self.google_check:\n",
    "            google_df = google(self.keyword)\n",
    "            if len(google_df):\n",
    "                df = pd.concat([df,google_df])\n",
    "        elif self.naver_check and self.google_check:\n",
    "            naver_df = naver(self.keyword)\n",
    "            google_df = google(self.keyword)\n",
    "            if len(naver_df) or len(google_df):\n",
    "                df = pd.concat([df,naver_df,google_df])\n",
    "        if self.other:\n",
    "            digital_df = digital_news()\n",
    "            digital_df2 = newspim()\n",
    "            digital_df3 = AItimes()\n",
    "            df=pd.concat([df,digital_df,digital_df2,digital_df3])\n",
    "        if len(df)>0:\n",
    "            df.to_excel('merge_data.xlsx',index=False)\n",
    "            root2 = tk.Tk()\n",
    "            msg = messagebox.showinfo(title='Complete',message='Complete!')\n",
    "            if msg == 'ok':\n",
    "                root2.destroy()\n",
    "        else:\n",
    "            root2 = tk.Tk()\n",
    "            msg = messagebox.showerror(title='Nothing',message='Nothing')\n",
    "            if msg == 'ok':\n",
    "                root2.destroy()\n",
    "if __name__ == \"__main__\" :\n",
    "    app = QApplication(sys.argv)\n",
    "    myWindow = WindowClass()\n",
    "    myWindow.show()\n",
    "    app.exec_()\n",
    "    app.exec_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49eba9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install naver_google"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
