{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21506a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pandas import DataFrame\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "from urllib.request import Request, urlopen # url open\n",
    "import urllib.parse\n",
    "\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5354e834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파파고 api 메소드 (글자수 때매 사용 안함)\n",
    "\n",
    "def get_translate(text):\n",
    "    client_id = \"zU05PnavleWtiyNb8xrJ\" # <-- client_id 기입\n",
    "    client_secret = \"GGWHMfmwyH\" # <-- client_secret 기입\n",
    "\n",
    "    data = {'text' : text,\n",
    "            'source' : 'en',\n",
    "            'target': 'ko'}\n",
    "\n",
    "    url = \"https://openapi.naver.com/v1/papago/n2mt\"\n",
    "\n",
    "    header = {\"X-Naver-Client-Id\":client_id,\n",
    "              \"X-Naver-Client-Secret\":client_secret}\n",
    "\n",
    "    response = requests.post(url, headers=header, data=data)\n",
    "    rescode = response.status_code\n",
    "\n",
    "    if(rescode==200):\n",
    "        send_data = response.json()\n",
    "        trans_data = (send_data['message']['result']['translatedText'])\n",
    "        return trans_data\n",
    "    else:\n",
    "        print(\"Error Code:\" , rescode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4f41296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv 파일 만들기 위해서\n",
    "\n",
    "date = str(datetime.now()) \n",
    "date = date[:date.rfind(':')].replace(' ', '_') \n",
    "date = date.replace(':','시') + '분' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b8f0f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "August\n"
     ]
    }
   ],
   "source": [
    "# 베트남 뉴스 크롤링 할 때, 월로 비교\n",
    "# 같은 달이면 크롤링, 달이 다를 경우 크롤링 스톱\n",
    "# 입력하는 형태로 할지 고민 중\n",
    "month = datetime.now()\n",
    "this_month = month.month\n",
    "print(this_month)\n",
    "month = month.strftime('%B')\n",
    "print(month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9b75e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그냥 열면 403 에러 발생\n",
    "# mod_security 또는 다른 비슷한 서버 시큐리티가 알려진 사용자 봇을 블록 시킴\n",
    "\n",
    "# 시작 페이지\n",
    "    \n",
    "page = '1'\n",
    "\n",
    "urlTicker = Request('https://www.khmertimeskh.com/category/business/page/' + page, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "webpage = urlopen(urlTicker).read()\n",
    "soup = BeautifulSoup(webpage, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2b6945",
   "metadata": {},
   "outputs": [],
   "source": [
    "#엑셀 만들 때 사용\n",
    "news_dict = {} \n",
    "idx = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1434fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기 안 돌려도 됨\n",
    "\n",
    "news_date = soup.find(class_='item-meta')\n",
    "news_date2 = news_date.find_all(class_='item-time')\n",
    "#print(news_date)\n",
    "\n",
    "print(news_date2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#이걸로\n",
    "test = soup.select('time')\n",
    "\n",
    "#print(test)\n",
    "\n",
    "print(len(test))\n",
    "\n",
    "\n",
    "for i in test:\n",
    "    print(i)\n",
    "    #print(i.attrs[datetime])\n",
    "\n",
    "\n",
    "#날짜 완료\n",
    "month2 = test[5:7]\n",
    "\n",
    "#test3 = soup.find_all(class_=\"item-time\")['datetime']\n",
    "\n",
    "\n",
    "#print(month2)\n",
    "\n",
    "check = str(this_month).zfill(2)\n",
    "#print(str(this_month).zfill(2))\n",
    "\n",
    "#print(month2 == check)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "beeb9b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More than 230,000 tourists recorded in Kingdom amid bad weather\n",
      "August 16, 2022\n",
      "https://www.khmertimeskh.com/501132941/more-than-230000-tourists-recorded-in-kingdom-amid-bad-weather/\n",
      "As rainfall, strong winds and floods lash the country, Cambodia continues to see tourist numbers well above the 200,000 mark – a positive sign for the recovering tourism sector.The Ministry of Tourism reported yesterday that last weekend, 13 to 14 August, the Kingdom saw 233,933 tourists. The Ministry report stated that 212,313 tourists were nationals and 21,620 are foreigners.The main tourist destinations in the country, included:Siem Reap with 38,249 tourists,Preah Sihanouk with 31,589 tourists,Kampot recorded 23,819 tourists,Battambang recorded 21,177 tourists,Kampong Speu recorded 18,826 tourists,Phnom Penh with 18,359 tourists andKep with 12,720 tourists.In the past four to five weeks, the flow of tourists in the country has fluctuated due to the weather conditions this rainy season. Floods and strong rainfall in certain areas has deterred prospective tourists to avoid travelling. The coastal regions, which is one of the main tourist destinations, are also experiencing rough seas and tourists and locals are warned about the danger of going out to sea.The Ministry of Tourism is discussing with relevant ministries, institutions and the private sector about the possibility of creating special tour packages to attract more tourists and a second evaluation of the quality of tourism and hospitality services.\n",
      "Error Code: 429\n",
      "None\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'news_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28mprint\u001b[39m(Korean_content) \u001b[38;5;66;03m# 한국어 번역\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 64\u001b[0m     \u001b[43mnews_dict\u001b[49m[idx] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m : newsTitle,\n\u001b[0;32m     65\u001b[0m                       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m : newsDate,\n\u001b[0;32m     66\u001b[0m                           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m'\u001b[39m : newsURL,\n\u001b[0;32m     67\u001b[0m                          \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m : real_content}\n\u001b[0;32m     69\u001b[0m     idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     71\u001b[0m page \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(page)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'news_dict' is not defined"
     ]
    }
   ],
   "source": [
    "# for문 돌면서 크롤링\n",
    "first_check = 1\n",
    "\n",
    "while True:\n",
    "    \n",
    "    \n",
    "    # 전체 돌릴 때 이걸로 해야 함\n",
    "    link = soup.find_all(class_='item-content')\n",
    "    \n",
    "    \n",
    "    #if  달이 다르면 종료\n",
    "    if page == '11':   # 한 페이지당 30개씩 10 페이지 크롤링\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "    for i in link:\n",
    "        \n",
    "        \n",
    "        newsTitle = i.find(class_='item-title').get_text()\n",
    "        newsDate = i.find(class_='item-time').get_text()\n",
    "        newsURL = i.find(\"a\")[\"href\"]\n",
    "        \n",
    "        \n",
    "        insideURL = Request(newsURL, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        webpage2 = urlopen(insideURL).read()\n",
    "        soup2 = BeautifulSoup(webpage2, 'html.parser')\n",
    "        #print(soup2)\n",
    "        \n",
    "        contents2 = soup2.find(class_='col-md-8 col-sm-12 col-xs-12')\n",
    "        \n",
    "        # 이전글과 다음글 태그 제거\n",
    "        if first_check == 1:\n",
    "            contents2.find(class_=\"entry-navigation-title\").decompose()\n",
    "            first_check += 1\n",
    "        else:\n",
    "            contents2.find(class_=\"entry-navigation-title\").decompose()\n",
    "            contents2.find(class_=\"entry-navigation-title\").decompose()\n",
    "            \n",
    "\n",
    "        \n",
    "        content = contents2.find_all(\"p\")\n",
    "        \n",
    "        \n",
    "        #본문 담을 변수\n",
    "        real_content =''\n",
    "\n",
    "        for test in content:\n",
    "\n",
    "            real_content = real_content + test.get_text()\n",
    "        \n",
    "        \n",
    "        \n",
    "        # for문 종료 및 출력 (테스트 출력)\n",
    "######  해당 변수로 워드 클라우드 및 나머지 진행 하면 됨!!!!!!!!!!!!!!1\n",
    "        print(newsTitle)    # 제목\n",
    "        print(newsDate)     # 날짜\n",
    "        print(newsURL)      # url\n",
    "        print(real_content) # 본문\n",
    "        Korean_content = get_translate(real_content)\n",
    "        print(Korean_content) # 한국어 번역\n",
    "        print('\\n')\n",
    "        \n",
    "        news_dict[idx] = {'title' : newsTitle,\n",
    "                          'date' : newsDate,\n",
    "                              'url' : newsURL,\n",
    "                             'content' : real_content}\n",
    "\n",
    "        idx += 1\n",
    "        \n",
    "    page = int(page)\n",
    "    page += 1\n",
    "    page = str(page)\n",
    "    print(page)  # page 체크라 지워도 됨\n",
    "        \n",
    "    urlTicker = Request('https://www.khmertimeskh.com/category/business/page/' + page, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    webpage = urlopen(urlTicker).read()\n",
    "    soup = BeautifulSoup(webpage, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7960491",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "크롤링 완료\n",
      "데이터프레임 변환\n",
      "엑셀 저장 완료 | 경로 : C:\\Users\\Cyber\\Crawling\\NewsCrawling\\캄보디아 뉴스_2022-08-12_14시53분.xlsx\n"
     ]
    }
   ],
   "source": [
    "print('크롤링 완료')\n",
    "\n",
    "print('데이터프레임 변환')\n",
    "news_df = DataFrame(news_dict).T\n",
    "\n",
    "folder_path = os.getcwd()\n",
    "xlsx_file_name = '캄보디아 뉴스_{}.xlsx'.format(date)\n",
    "\n",
    "news_df.to_excel(xlsx_file_name)\n",
    "\n",
    "print('엑셀 저장 완료 | 경로 : {}\\\\{}'.format(folder_path, xlsx_file_name))\n",
    "os.startfile(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "956e2296",
   "metadata": {},
   "outputs": [],
   "source": [
    "import  pyautogui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28440f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한글 번역\n",
    "\n",
    "pyautogui.hotkey('shift', 'F10')\n",
    "for i in range(9):\n",
    "    pyautogui.hotkey('down')\n",
    "pyautogui.hotkey('enter')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3f10a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
